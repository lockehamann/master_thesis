
\chapter{引言}
\section{研究背景与意义}

自2011年4月日本福岛地震及其引发福岛第一核电站受损以来，
核能的安全性再次被世界各方所关注，
整个核能行业乃至全世界对核能安全性的要求进一步提高，
这给反应堆物理方向带来了更高的要求：
更快的计算速度和更高的计算精度。

一直以来，反应堆物理计算中最为主要的中子输运方程的求解方法
分为两条线：对输运方程进行简化并离散为线性方程组相关问题的确定论方法
和直接使用蒙特卡罗方法对中子输运方程及实际中子反应过程进行联合求解的蒙特卡罗方法。
对比这两种路线，前者倾向于简化问题来获得更快的计算速度，
后者则利用蒙特卡罗方法对高维偏微分方程的直接求解能力追求尽可能高的精度。

蒙特卡罗方法虽然在理论上有着极高的计算结果精度，
但同样面临着计算时间较长、误差具有随机性、
提高计算精度的代价巨大等等问题。
这方面的代表程序之一是有着悠久历史的MCNP程序\cite{forster2004mcnp}，
至今已发展到MCNP5；
国内近几年也有不少蒙特卡罗程序正在开发，
比较有代表性的如清华大学工程物理系核能所开发的
针对反应堆物理计算的蒙特卡罗程序RMC\cite{li2010development}。
在实际计算中，虽然蒙特卡罗方法需要的计算成本相当高，
但却有着天然的粒子并行特性，适合多机多节点的超级计算机并行计算，
近些年随着大型计算机技术的不断发展，
蒙卡方法也获得了能够在较短的时间内较为精细地计算全堆级问题的计算能力，
如文献\onlinecite{she2013development}。

而传统的确定论方法则主要有三个分支：
不考虑角度的扩散近似方法，$P_N$、$S_N$等角度离散方法，
能够处理任意几何的特征线方法。
前两种方法在空间离散方式大多使用有规则网格、有限元不规则网格甚至自适应结构网格\cite{wang2009three}，
而特征线方法在理论上可以支持任意几何。
在能量方面，确定论方法一般只做分群近似，
非几何、角度方面的因素则大多在群截面构造时进行处理。
即使做了上述各种近似，确定论方法面对全堆问题时仍然有计算量、
存储量太大的问题\cite{azmy1997multiprocessing}，
所以又在此基础上发展了各种节块方法来减少空间网格数量，
来解决计算量、存储量过大的问题。
目前工程中全堆计算大多采用组件--堆芯两步法，
即先用细群输运类方法对全反射边界的组件进行求解，
得到组件的均匀化少群截面后，再求解三维少群扩散问题。
两步法的问题在于求解时使用的组件的边界条件不准确，由此引入了一定的误差。
随着对计算精度要求的不断提高，
全堆问题的一步细网直接求解又重新进入了人们的视野。

而全堆问题一步细网求解的主要困难在于离散后的线性方程组的未知元数量太大，
无论是存储还是求解都很难在单机上完成。
一直以来，这种大规模线性方程的求解或是矩阵特征值的计算都依赖于大型机或超级计算机
等多节点分布式计算系统。
但随着个人计算机技术的不断发展，传统用于PC显示加速的图形加速卡（现一般称为显卡）
的计算能力和IO（Input/Output）吞吐能力的不断增强，
已经能够满足某些科学计算或通用计算的需求，由此产生了GPU计算，
GPU是指显卡的核心部分——图形处理单元。
例如2013年Nvidia发布民用级显卡Geforce GTX Titan的
峰值单精度浮点计算能力已经高达4.7TFlop/s，
远高于现在CPU单芯片的计算能力，
而2000年6月的TOP500 Super Computer \cite{meuer2001top500}
排行世界第一的超级计算机的峰值浮点性能也只有3.2TFlop/s
（见 \url{http://www.top500.org/site/48748}）。
如何利用计算机科学界提供的这份礼物就成了各个行业的科学计算工作者所共同面对的问题。

\section{国内外研究现状}

\subsection{GPU通用计算}

\subsubsection{GPU发展历史简述}
GPU全称Graphics Processing Unit，译作图形处理单元，
是现在个人电脑中显卡的核心部分。
显卡最初是作为图形加速卡出现的，所以GPU和CPU不同，
是专门为图形处理中大量重复计算的处理进行设计的。

GPU的编程方式最初由GPU的生产厂商决定，
缺乏统一的标准\footnote{现在的手机GPU正处于这样的状态。}。
为了统一众多的标准，先后出现了两套的图形编程API(Application Programming Interface)：
微软的DirectX和开放的OpenGL，现在这两套API已经被广泛接受和认可。
DirectX和OpenGL是针对图形应用的API，
所以其中支持的运算也都是图形处理领域的运算，
并不适合一般的科学计算和通用计算。
各公司为了追求利润，GPU硬件本身也是针对DirectX/OpenGL进行设计，
初期GPU内部划分为若干种特化的运算单元。

随着用户需求的不断提高和技术的不断发展，
在GPU中实现各种功能单一的特化运算单元遇到了一些问题：
一是各种单元的比例分配，不同的游戏对于不同的硬件资源有着不同的需求，
在设计时固化一种比例很难适应所有游戏的需要，并会造成计算资源的浪费；
二是游戏开发人员深入控制、定制图形处理过程的需求也在不断增长，
特化的硬件不能适应这种需求。
所以从2001年左右开始，GPU的硬件设计由特化的运算单元转为通用运算单元，
DirectX/OpenGL等也提供了通用编程的方式。
这为GPU的通用计算打下了基础。

\subsubsection{GPU通用计算历史简述}

早在DirectX/OpenGL提供通用编程方式以前，
就有科学工作者试图在GPU上进行科学计算等非图形计算，
其主要方式为：把计算任务转化或伪装为一个普通的图形计算任务，
交给图形API进行计算。这种方式对能够转化的计算任务的类型有着相当大的限制，
而且这种转化过程较为繁琐，对编程人员的技术要求较高，
最后该方法并未得到广泛的使用。

等DirectX/OpenGL提供通用编程方式以后，虽然各种限制有所放松，
但由于DirectX/OpenGL本身并非针对通用计算设计，
所以对于一般的科学工作者来说，编程仍非常不便。

直到2007年NVIDIA公司提出了CUDA(Compute Unified Device Architecture)编程模型，
并进行大力推广后，GPU的通用计算才逐渐走入科学工作者的视野。
不过，目前除了某些对计算能力要求特别高的领域以外，
GPU进行通用计算仍在学术界领域，尚未大量进入工业界，
GPU通用计算和科学计算的各项基础技术、基础设施也尚在不断发展、完善中。

到目前为止，CUDA仍然是最成熟的GPU通用计算和科学计算的解决方案，
但也有很多新技术标准，如OpenCL、OpenACC等
（将在\sectionref{sec:gpu.other}进行介绍）。

而多GPU\cite{LeviGTC2013}、
MPI+CUDA\cite{Jiri2013}、GPU集群\cite{StefanGTC2013}等
多GPU多节点混合并行技术尚在不断发展中。

\subsubsection{线性代数算法的GPU并行化}

线性代数是科学计算中的主要基础工具之一，在各种科学计算问题中大量存在，
而且大规模线性代数问题的求解是很多问题求解的瓶颈之一，
所以线性代数相关算法的GPU并行化很早就被各界所重视。

现在，线性代数方面的研究按照矩阵的稀疏性分为两个方向：
稠密矩阵线性代数的GPU加速、稀疏矩阵线性代数的GPU加速。
目前，稠密矩阵线性代数的GPU加速研究的相对成熟，对于单节点计算条件
GPU单芯性能可达CPU单芯的7倍\footnote{测试环境：CPU组为Intel Sandy Bridge E5-2687W 8核 3.1GHz，
计算程序MKL 10.3.6；GPU组为 K20X，软件环境cuBLAS 5.0，不记数据传输时间。}\cite{WillGTC2013}，
多节点多GPU的稠密矩阵线性代数库如MAGMA\cite{horton2011class}也已经较为成熟。

稀疏矩阵线性代数的性能在单节点条件下可达12倍\footnote{计算条件同前。}，
但对于稀疏矩阵线性方程的求解尚在不断研究、发展中\cite{KyleGTC2013}，
尚未达到稠密线性代数那样进展到多节点多GPU的程度。

\subsection{反应堆物理GPU并行化研究}

\subsubsection{确定论方法的GPU并行化}

使用GPU加速确定论方法的工作最早见于2010年，Prayudhatama D等人\cite{prayudhatama2010gpu}
首先对一维扩散程序的GPU并行化进行了探索，相对于文中提供的CPU对比程序获得了最高70倍的加速。
需要说明的是，比较同样算法在不同硬件上的速度并没有太多工程实用上的意义，
因为处理器结构的不同会影响到算法的效果，一般来讲每种处理器会有自己的最优算法，
在实际应用中不同处理器上的最佳算法间的比较才能反映技术进步给科学工作者、工程师等用户带来的实际改善效果。
此后2011年，Kodama Yasuhiro等人\cite{kodamastudy}
对实际扩散程序SCOPE2进行了GPU并行化，取得了3倍加速效果。
同年，清华大学工程物理系的徐琪等人\cite{xuqi_gpu_old}也对三维扩散程序进行GPU并行化研究，
相对传统的CPU单单线程程序Citation取得了3-5倍的加速效果。

同样在2011年，国内龚春叶等人\cite{gong2011gpu, gong2012particle, gongyechun}
对三维$S_N$方法的GPU并行化进行了研究，
对于三维$S_N$方法相对于文中提供的对比程序有2-8倍左右的加速。
2012年，Oak Ridge National Laboratory研究了\cite{baker2012high}GPU超级计算机上的
三维$S_N$方法大规模并行化，实现了35TFlop/s的计算能力，
同时指出$S_N$方法的并行可扩展性较差。

此外文献\onlinecite{jamelot2011high,kirschenmann2011parallel,kodamastudy}
等对三维$\mathrm{S}P_N$方法的并行化进行了研究，
相对于他们的CPU串行实现取得了最高36的加速。

2013年，Talamo Alberto\cite{talamo2013numerical}和
清华大学的张知竹\cite{zhangzhizhu}实现了三维特征线方法的GPU加速，
后者最高取得了将近100倍的加速效果。

\subsubsection{蒙特卡罗方法的GPU并行化}

2009年，Nelson Adam Gregory\cite{nelson2009monte}较早地研究了
蒙特卡罗方法的GPU并行化，取得了11-20倍的加速效果。
此后2011年，国防科学技术大学的龚春叶宣称实现了MCNP的GPU并行化\cite{gongyechun}，
并实现了10倍左右的加速。
近些年来蒙卡方法的GPU并行化仍在不断研究中，
相关的文献如\onlinecite{ding2011evaluation,willert2012hybrid,
liu_monte_2012,marcus2012mcmini,xuqians}等，
结果大多加速比有限或对模型有简化处理。

蒙特卡罗算法的主要问题是，程序行为是随机的，
很难对其进行静态预测，此外还有不规则的内存访问、
控制流动态不规则改变、行为和并行度显著依赖于输入等特点，
这些都给蒙特卡罗方法的GPU并行化带来了较大困难，
导致直接进行GPU移植的效果很差。\cite{Martin2013}
蒙特卡罗方法的GPU并行化想要取得较好的成果可能需要对
传统计算方式做较大的修改。\cite{lichenglong}


\subsection{数值加速方法}

除了硬件加速外，还可以使用数值计算等领域的研究成果加速线性方程组的求解或本征值计算。
对于扩散方程，早在1989年Suetomi Eiichi等人\cite{suetomi1989conjugate}
就尝试对二维圆柱几何应用共轭梯度法CG(Conjugate Gradient)，
并研究了不同预条件算法对收敛次数的影响。
此后Suetomi Eiichi\cite{suetomi1991conjugate}和Gupta Anurag\cite{gupta_krylov_2004}
分别研究了CG等Krylov类方法对于扩散计算中k本征值问题的求解过程，并对源迭代过程进行了改进。
1998年，Thomas J. Downar等人\cite{so1998mapping}对于二维扩散问题研究了
CG方法在SIMD、MIMD等向量机、并行机上的性能。

除了迭代算法外，Ginestar D\cite{ginestar2001multilevel}等人提出了Multilevel方法，
即通过在粗网上进行预先求解来给细网求解过程提供一个更为精确的通量分布初值，
利用粗网格求解代价较小的特点来加速细网计算。

除了数值计算的加速方法外，反应堆物理计算中还结合物理模型的特点进行加速求解，
输运问题如$S_N$方法或特征线方法虽然也离散为线性方程组，
但由于其物理特性有中子飞行方向的概念，
对于固定方向往往可以按某种空间网格顺序依次解出，
即系数矩阵是分块三角的，
一般求解过程中会利用这种特性，而不是直接套用通用方法求解。
但这种类似于高斯消去法中回代操作的过程串行度很高，
即各元素间的计算次序依赖很多，不利于GPU并行。
此外，对于输运问题，Alcouffe Raymond E\cite{alcouffe1977diffusion}提出了使用扩散方法加速
输运计算的扩散综合加速DSA(Diffusion Synthetic Acceleration)方法。

近些年中反应堆物理领域中还大量使用了各种节块类方法，
其主要思想是通过提高每个空间网格对通量分布的描述能力来实现同等精度下
大量减少空间网格数目，但节块法不能直接产生细网网格上的通量分布，
需要通过通量重构等方法产生，这会引入一定的误差。

\subsection{时空动力学与实时仿真}

时空动力学研究的是反应堆中中子通量如何随时间进行变化，
即在稳态输运、扩散等方程中添加时变项，并考虑缓发中子，
得到一个含时的偏微分方程。
对于该方程可以直接进行离散，可以逐个时间步依次地求解整个问题。
但由于时空动力学问题本身的刚性较强，
时间离散使用显式差分格式需要把时间步取得很小才能使结果收敛于实际结果，
所以一般使用隐式格式来减少时间步的数量，
这样对于每个时间步都需要求解一个由之前时间步的状态确定的一个固定源问题，
即每步求解一次扩散或输运方程。
为了进一步降低计算量，出现了绝热近似、准静态近似、改进的准静态近似等方法，
主要思路是将通量分布的形状变化（以下称为形状函数）和幅度变化解耦，
由于形状函数变化较慢，可以以较低的频率进行更新，
而对于变化较快的通量幅度则用较高的频率进行更新，
由于通量幅度的更新代价较低，所以最终实现较少计算量的目的。
\cite{huangzuqia2007}

无论是直接求解还是以准静态为代表的解耦方法，都需要以一定的周期求解
全堆的扩散、输运方程，但由于扩散、输运方程求解的代价很高，
对于大规模问题想要达到实时仿真在过去基本是不可能的。
直至近些年，基于扩散的准静态节块时空动力学才达到实时：
1996年齐克林等人\cite{qikelin1996}对于1452个网格0.1s时间步长的问题实现实时仿真，
2010年丁小川等人\cite{dingxiaochuan2011}通过对传统的交替方向隐式（ADI）方法进行了改进后，
实现了13440个网格0.25s时间步长的实时仿真。
除了扩散、输运外，Taylor J. B等人\cite{taylor2007development}于2007年研究了三维特征线时空动力学
求解，但距实时仿真仍有较远距离。


\section{研究内容和论文组织结构}

本文共分为六章。

第1章为引言，介绍课题背景需求及意义，当前GPU通用计算技术发展情况、
反应堆物理方面的GPU加速工作、其他数值加速方法和时空动力学及实时仿真相关的国内外研究进展。

第2章介绍GPU科学计算相关的技术，主要介绍CUDA技术及GPU上线性方程求解算法。

第3章介绍\ProgramName 的开发过程，包括理论公式推导、程序主体结构和相关的技术路线选择。

第4章对\ProgramName 程序的结果进行了验证，并分析了\ProgramName 的加速效果。

第5章在\ProgramName 程序的基础上讨论并分析了影响程序性能的一些因素。

第6章给出本论文工作的结论和展望。