
\chapter{引言}
\section{研究背景与意义}

自2011年4月日本福岛地震及其引发福岛第一核电站受损以来，
核能的安全性又再次为世界各方所关注，
全世界及整个核能行业对核能安全性的要求进一步提高，
处在核能行业内的反应堆物理方向也面临着
不断提高的计算精度和计算速度的巨大需求。

一直以来，反应堆物理计算中最为主要的中子输运方程的求解方法
分为两条线：对输运方程进行简化并离散为线性方程组相关问题的确定论方法
和直接使用蒙特卡罗方法对中子输运方程及实际中子反应过程进行联合求解的蒙特卡罗方法。
后者主要依赖于蒙特卡罗方法对高维偏微分方程的直接求解能力，
但同样也面临着蒙特卡罗方法计算时间较长、误差具有随机性、
提高计算精度的代价巨大等等问题。
这方面的代表程序是有着悠久历史的MCNP程序\cite{forster2004mcnp}，
至今已发展到MCNP5，不久后MCNP6也将发布；
国内近几年也有不少蒙特卡罗程序正在开发，
比较有代表性的是清华大学工程物理系核能所开发的
针对反应堆物理计算的蒙特卡罗程序RMC\cite{li2010development}。
由于蒙特卡罗方法需要的计算成本相当高，
但却有相当高的天然粒子并行特性，适合多机多节点并行计算，
所以直至近些年计算机技术充分发展后才具有全堆计算能力，
目前蒙特卡罗程序全堆精细计算仍常在大型机或超级计算机上进行，
如文献\onlinecite{she2013development}。

确定论方法方面有三个主要分支：
不考虑角度的扩散近似方法，$P_N$、$S_N$等角度离散方法，
能够处理任意几何的特征线方法。
这些方法在空间上大多使用有规则网格或有限元不规则网格的空间离散方式。
也有自适应结构网格方面的工作，如文献\onlinecite{wang2009three}，
研究了三维扩散方程的自适应结构网格方法。
虽然特征线方法在理论上可以支持任意几何，
但不少这方面的工作由于工作量或加速方面的原因而对几何模型进行限制。
在能量方面，确定论方法一般只做分群近似，
非几何、角度方面的因素则大多交给群截面构造方面处理。
即使做了上述各种近似，确定论方法面对全堆问题时仍然面临计算量、
存储量太大的问题，\cite{azmy1997multiprocessing}
所以在此基础上发展了各种节块方法来减少空间网格数量。
目前工程中全堆计算大多采用组件-堆芯的两步法，
即先用细群输运对全反射边界组件进行求解，得到组件的均匀化少群截面，再由三维堆芯程序求解。
两步法的问题在于求解时使用的组件的边界条件不准确，由此带来误差。
确定论方法面对堆芯级问题的细网直接求解仍然未见较为完整的解决方案。
而随着对于反应堆安全的关注越来越多，要求越来越高，
对于细网直接求解的需求也在不断增加。

一直以来，大规模线性方程的求解和矩阵特征值的计算都依赖于大型机或超级计算机
等多节点分布式计算系统才能在有效的时间内进行求解，
但随着个人计算机技术的不断发展，传统用于显示加速的图形加速卡（现一般称为显卡）
的计算能力和IO（Input/Output）能力的不断增强，
使其已经能够满足某些科学计算或通用计算的需求。
由此发展出了GPU并行化路线，GPU是指显卡的核心部分——图形处理单元。
2013年，民用显卡Nvidia Geforce GTX Titan的峰值单精度浮点计算能力已经高达4.7TFlop/s，
远高于现在CPU单芯片的计算能力。
而2000年6月的TOP500 Super Computer \cite{meuer2001top500}
排行世界第一的超级计算机的峰值浮点性能也只有3.2TFlop/s
（见 \url{http://www.top500.org/site/48748}）。
如何利用这样强大的计算能力的问题已经早就摆在了各个行业的面前。

\section{国内外研究现状}

\subsection{反应堆物理GPU并行化研究}
\subsubsection{确定论方法的GPU并行化}

确定论的GPU并行化最早见于2010年，
文献\onlinecite{prayudhatama2010gpu}首先对
一维扩散程序的GPU并行化进行了探索。%相对于文中提供的CPU对比程序获得了最高70倍的加速。
此后文献\onlinecite{kodamastudy}和\onlinecite{xuqi_gpu_old}（国内）分别对
三维扩散的GPU并行化进行了研究，
相对于实际CPU扩散程序取得了3-5倍的加速效果。

2011年，国内龚春叶等人\cite{gong2011gpu,gong2012particle,gongyechun}
和文献\onlinecite{jamelot2011high}
对三维$S_N$方法的GPU并行化进行了研究，
前者对于三维$S_N$方法相对于文中提供的对比程序有2-8倍左右的加速。
2012年Oak Ridge National Laboratory研究了GPU超级计算机上的
三维$S_N$方法大规模并行化，实现了35TFlop/s的计算能力，
同时指出$S_N$方法的并行可扩展性较差。\cite{baker2012high}

此外文献\onlinecite{jamelot2011high,kirschenmann2011parallel,kodamastudy}
等对三维$\mathrm{S}P_N$方法的并行化进行了研究，
相对于他们的CPU串行实现取得了最高36的加速。

2013年，文献\onlinecite{talamo2013numerical}和\onlinecite{zhangzhizhu}（国内）实现了
三维特征线方法的GPU加速，后者最高取得了将近100倍的加速效果。

\subsubsection{蒙特卡罗方法的GPU并行化}

2009年，文献\onlinecite{nelson2009monte}较早地研究了
蒙特卡罗方法的GPU并行化，并取得了11-20倍的加速效果。
此后国内的龚春叶实现了MCNP的GPU并行化\cite{gongyechun}，
实现了10倍左右的加速。
近些年来蒙卡方法的GPU并行化仍在不断发展中，
相关的文献较多，如\onlinecite{ding2011evaluation,willert2012hybrid,
liu_monte_2012,marcus2012mcmini,xuqians}等，
结果大多加速比有限或对模型有简化处理。
蒙特卡罗算法的主要问题是，程序行为是伪随机的，
很难对其进行静态预测，此外还有：不规则的内存访问，
控制流动态不规则改变，行为和并行度显著依赖于输入等特点。
这些都给蒙特卡罗方法的GPU并行化带来了较大困难，
导致直接进行GPU移植的效果很差。\cite{Martin2013}
蒙特卡罗方法的GPU并行化想要取得较好的成果可能需要对
传统计算方式做较大的修改。\cite{lichenglong}

\subsection{数值加速方法}

确定论方法将中子输运方程离散为线性方程组后，
就需要对其进行求解，对于三维问题，方程组的带求解变量数很大，
为了在合理的时间内进行求解则势必进行加速。

扩散方法离散后的方程为主对角占优线性方程的求解或本征值计算，
可使用数值计算领域的研究成果。输运问题如$S_N$方法或特征线方法
虽然同样离散为线性方程组，但由于其物理特性有中子飞行方向的概念，
对于固定方向往往可以按某种空间网格顺序依次解出，
即系数矩阵是分块三角的。
一般求解过程中会利用这种特性，而不是直接套用通用方法求解，
但遗憾的是这种类似于高斯消去法中回代操作的过程串行度很高，
即各元素间的计算次序依赖很多，不利于GPU并行。
对于输运问题，文献\onlinecite{alcouffe1977diffusion}提出了使用扩散方法加速
输运计算的扩散综合加速DSA(Diffusion Synthetic Acceleration)方法。

除此以外，反应堆物理领域中还大量使用了各种节块类方法，
其主要思想是通过提高每个空间网格对通量分布的描述能力来实现同等精度下
大量减少空间网格数目，但节块法不能直接产生细网网格上的通量分布，
需要通过通量重构等方法产生，这都引入了一定程度的误差。

对于扩散方程，早在1989年文献\onlinecite{suetomi1989conjugate}
就尝试对二维圆柱几何应用共轭梯度法CG(Conjugate Gradient)，
并研究了不同预条件算法对收敛次数的影响。
文献\onlinecite{suetomi1991conjugate}和\onlinecite{gupta_krylov_2004}
分别研究了CG等Krylov类方法对于扩散计算中k本征值问题的求解，
并对源迭代过程进行了改进。
1998年，文献\onlinecite{so1998mapping}对于二维扩散问题研究了
CG方法在SIMD、MIMD等向量机、并行机上的性能。

除了迭代算法外，文献\onlinecite{ginestar2001multilevel}提出了
Multilevel方法，即通过在粗网上进行预先求解来给细网求解过程提供一个更为精确的通量分布初值，
利用粗网格求解代价较小的特点来加速细网计算。

\subsection{动力学}

扩散动力学实时，三维13440个网格,0.25s实时。\cite{宋英明2010}

三维节块准静态，1936个节块，最高55倍运行。\cite{丁小川2011}

三维准静态，1452个网格，对于0.1s达到实时。\cite{齐克林1996}


\subsection{temp}
对于稠密线性代数计算GPU单芯性能可达CPU单芯的7倍，
稀疏线性代数性能更是高达12倍。\footnote{测试环境：CPU组为Intel Sandy Bridge E5-2687W 8核 3.1GHz，
计算程序MKL 10.3.6；GPU组为 K20X，软件环境cuBLAS 5.0，不记数据传输时间。}
\cite{WillGTC2013}

GPU上稀疏矩阵的预条件算法和直接求解算法尚在不断研究、发展中。\cite{KyleGTC2013}

目前GPU上对角线方程求解研究的较为充分的仍然是三对角线方程组\cite{LiWenChangGTC2013}

多GPU\cite{LeviGTC2013}、
MPI+CUDA\cite{Jiri2013}、GPU集群\cite{StefanGTC2013}等
多GPU多节点混合并行技术也在不断发展中。

\section{研究内容和论文组织结构}
